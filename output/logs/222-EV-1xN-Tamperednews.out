Traceback (most recent call last):
  File "/nfs/home/ernstd/masterthesis_scripts/./experiments/02_with_comparative_images/22_multi_input_models/222_entity_verification_1xN/scripts/tamperednews/prepare_questions.py", line 95, in <module>
    createSingleEntityQuestions(args)
  File "/nfs/home/ernstd/masterthesis_scripts/./experiments/02_with_comparative_images/22_multi_input_models/222_entity_verification_1xN/scripts/tamperednews/prepare_questions.py", line 68, in createSingleEntityQuestions
    saveQuestion(args, str(lineObject['id']), str(news_image), str(entity_image), str(counter), str(question), str(entityObject['name']), "orginal", "text", "no", "yes")
TypeError: saveQuestion() takes 10 positional arguments but 11 were given
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/nfs/home/ernstd/miniconda3/envs/transformer/lib/python3.12/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect
  warnings.warn(
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:35<01:11, 35.63s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:02<00:30, 30.57s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:31<00:00, 57.29s/it]Loading checkpoint shards: 100%|██████████| 3/3 [02:31<00:00, 50.58s/it]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Python version is above 3.10, patching the collections module.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [01:58<03:57, 118.62s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:59<00:49, 49.41s/it] Loading checkpoint shards: 100%|██████████| 3/3 [03:32<00:00, 69.46s/it]Loading checkpoint shards: 100%|██████████| 3/3 [03:32<00:00, 70.97s/it]
Traceback (most recent call last):
  File "/nfs/home/ernstd/masterthesis_scripts/./experiments/02_with_comparative_images/22_multi_input_models/222_entity_verification_1xN/scripts/tamperednews/printResultTable.py", line 65, in <module>
    printResults(args)
  File "/nfs/home/ernstd/masterthesis_scripts/./experiments/02_with_comparative_images/22_multi_input_models/222_entity_verification_1xN/scripts/tamperednews/printResultTable.py", line 49, in printResults
    getValue(resultsVLM[args.models[2]], entityType), 
                        ~~~~~~~~~~~^^^
IndexError: list index out of range
