Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.83s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.00it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.15it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:07,  1.52s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:03<00:07,  1.83s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:04<00:04,  1.58s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:06<00:02,  1.47s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:07<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:07<00:00,  1.32s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.11it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.15it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.44it/s]
Some weights of the model checkpoint at ./models/llava-v1.6-mistral-7b-hf/ were not used when initializing LlavaForConditionalGeneration: ['image_newline']
- This IS expected if you are initializing LlavaForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/nfs/home/ernstd/masterthesis_scripts/./experiments/3_image_entity_verification/scripts/tamperednews/printResultTable.py", line 65, in <module>
    printResults(args)
  File "/nfs/home/ernstd/masterthesis_scripts/./experiments/3_image_entity_verification/scripts/tamperednews/printResultTable.py", line 47, in printResults
    getValue(resultsVLM['instructBlip_answers'], entityType), 
             ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^
KeyError: 'instructBlip_answers'
